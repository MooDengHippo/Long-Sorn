import streamlit as st
import os
from dotenv import load_dotenv
from google.cloud import speech
import time
import pandas as pd
import subprocess # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ FFmpeg
import tempfile # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß

# --- Page Configuration ---
st.set_page_config(
    page_title="LongSorn AI Playground",
    page_icon="ü§ñ",
    layout="wide"
)

# --- Load Environment Variables ---
load_dotenv()

# --- Backend Functions (AI Calls) ---

def convert_audio_with_ffmpeg(input_bytes):
    """
    ‡πÉ‡∏ä‡πâ FFmpeg ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà STT ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    (WAV, 16-bit PCM, 16000 Hz, Mono)
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Input ‡πÅ‡∏•‡∏∞ Output
        with tempfile.NamedTemporaryFile(delete=False, suffix=".tmp") as temp_in:
            temp_in.write(input_bytes)
            input_filename = temp_in.name
        
        output_filename = input_filename + ".wav"

        # ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á FFmpeg
        command = [
            "ffmpeg",
            "-i", input_filename,      # Input file
            "-acodec", "pcm_s16le",    # Audio codec: 16-bit signed little-endian PCM
            "-ar", "16000",            # Audio sample rate: 16000 Hz
            "-ac", "1",                # Audio channels: 1 (Mono)
            "-y",                      # Overwrite output file if it exists
            output_filename
        ]
        
        process = subprocess.run(command, check=True, capture_output=True, text=True)
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Output ‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß
        with open(output_filename, "rb") as f:
            output_bytes = f.read()
            
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        os.remove(input_filename)
        os.remove(output_filename)
        
        return output_bytes, None
    except subprocess.CalledProcessError as e:
        # ‡∏Å‡∏£‡∏ì‡∏µ FFmpeg ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        error_message = f"FFmpeg error: {e.stderr}"
        st.error(error_message)
        return None, error_message
    except Exception as e:
        return None, str(e)


@st.cache_data
def run_stt_transcription(audio_file_content):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Google STT API ‡∏à‡∏£‡∏¥‡∏á
    """
    try:
        client = speech.SpeechClient()
        audio = speech.RecognitionAudio(content=audio_file_content)
        
        # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô WAV 16kHz ‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏ config ‡πÑ‡∏î‡πâ
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="th-TH",
            enable_automatic_punctuation=True,
            enable_word_time_offsets=True,
        )
        
        response = client.recognize(config=config, audio=audio)
        return response, None
    except Exception as e:
        st.error(f"Google STT API Error: {e}")
        return None, str(e)

def run_mock_nlp_analysis(transcript: str):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á NLP (Gemini/Typhoon)
    """
    time.sleep(1) # ‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏≥‡∏•‡∏≠‡∏á
    filler_word_count = transcript.lower().count("‡πÄ‡∏≠‡πà‡∏≠") + transcript.lower().count("‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤") + transcript.lower().count("‡∏≠‡∏∑‡∏°")
    return {
        "speech_analysis": {
            "Filler Words Detected": filler_word_count,
            "Speaking Pace": "Good",
            "Clarity Score": 8.2
        },
        "timeline_feedback": [
            {"timestamp": "0:01", "type": "Filler Word", "suggestion": "‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡πÄ‡∏≠‡πà‡∏≠' ‡∏•‡∏≠‡∏á‡πÄ‡∏ß‡πâ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡πÅ‡∏ó‡∏ô"},
            {"timestamp": "0:06", "type": "Filler Word", "suggestion": "‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤' ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢"},
        ],
        "ai_recommendations": [
            {"original": "‡πÄ‡∏≠‡πà‡∏≠... ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£...", "suggestion": "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á..."},
            {"original": "‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å", "suggestion": "‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å"}
        ]
    }

# --- Main UI ---

st.title("ü§ñ LongSorn AI Playground")
st.caption("‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI Pipeline ‡∏ó‡∏µ‡πà‡∏°‡∏µ UI ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏à‡∏£‡∏¥‡∏á")
st.divider()

# --- Upload ---
st.header("Upload Your Content")
uploaded_file = st.file_uploader(
    "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á (‡∏à‡∏≥‡∏Å‡∏±‡∏î 1 ‡πÑ‡∏ü‡∏•‡πå, ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 100MB)",
    type=["mp4", "mov", "mp3", "wav", "m4a", "flac"],
    label_visibility="collapsed"
)

if uploaded_file is not None:
    file_size_mb = uploaded_file.size / (1024 * 1024)
    if file_size_mb > 100:
        st.error("‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100MB ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà")
    else:
        st.info(f"Selected File: **{uploaded_file.name}** ({file_size_mb:.2f} MB)")
        st.audio(uploaded_file)
        
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("Upload & Analyze", type="primary", use_container_width=True):
                st.session_state.clear()
                st.session_state.analysis_triggered = True
                st.session_state.uploaded_file_content = uploaded_file.getvalue()
        with col2:
            if st.button("Clear", use_container_width=True):
                st.session_state.clear()
                st.rerun()

# --- Processing ---
if 'analysis_triggered' in st.session_state and st.session_state.analysis_triggered:
    with st.status("AI is analyzing your content...", expanded=True) as status:
        
        status.update(label="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô...")
        # --- ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ FFmpeg ‡∏Å‡πà‡∏≠‡∏ô ---
        converted_audio_content, ffmpeg_error = convert_audio_with_ffmpeg(st.session_state.uploaded_file_content)
        
        if ffmpeg_error:
            status.update(label="‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå!", state="error", expanded=True)
            st.stop()
        
        status.update(label="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Speech-to-Text)...")
        # --- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ STT ‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß ---
        stt_response, stt_error = run_stt_transcription(converted_audio_content)
        
        if stt_error:
            status.update(label="‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î!", state="error", expanded=True)
            st.stop()
        
        st.session_state.stt_response = stt_response
        status.update(label="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤ (AI Analysis)...")
        
        full_transcript = " ".join(
            [result.alternatives[0].transcript for result in stt_response.results if result.alternatives]
        ) if stt_response and stt_response.results else ""

        nlp_results = run_mock_nlp_analysis(full_transcript)
        st.session_state.nlp_results = nlp_results
        
        status.update(label="‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!", state="complete", expanded=False)

    st.session_state.analysis_triggered = False
    st.session_state.results_ready = True

# --- Results ---
if 'results_ready' in st.session_state and st.session_state.results_ready:
    st.divider()
    st.header("AI Analysis Results")
    stt_res = st.session_state.stt_response
    nlp_res = st.session_state.nlp_results
    
    left_col, right_col = st.columns(2, gap="large")
    with left_col:
        st.subheader("Presentation Playback")
        st.video("https://www.youtube.com/watch?v=dQw4w9WgXcQ")
        st.subheader("Timeline Feedback")
        for feedback in nlp_res["timeline_feedback"]:
            with st.container(border=True):
                r1_col1, r1_col2 = st.columns([1, 4])
                with r1_col1:
                    st.write(f"**{feedback['timestamp']}**")
                with r1_col2:
                    st.write(f"**{feedback['type']}**")
                st.info(f"**Suggestion:** {feedback['suggestion']}")
    
    with right_col:
        st.subheader("Speech Analysis")
        with st.container(border=True):
            analysis = nlp_res["speech_analysis"]
            st.metric("Filler Words Detected", f"{analysis['Filler Words Detected']} times")
            st.metric("Speaking Pace", analysis['Speaking Pace'])
            st.metric("Clarity Score", f"{analysis['Clarity Score']} / 10")
        
        st.subheader("Transcript & Word Timestamps")
        with st.expander("‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Transcript ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥"):
            if not stt_res or not stt_res.results or not stt_res.results[0].alternatives:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á")
            else:
                full_transcript = " ".join([res.alternatives[0].transcript for res in stt_res.results])
                st.text_area("Full Transcript", full_transcript, height=150)
                word_data = []
                for result in stt_res.results:
                    for word_info in result.alternatives[0].words:
                        word_data.append({
                            "Word": word_info.word,
                            "Start (s)": f"{word_info.start_time.total_seconds():.2f}",
                            "End (s)": f"{word_info.end_time.total_seconds():.2f}"
                        })
                st.dataframe(word_data)

    if st.button("Analyze Another"):
        st.session_state.clear()
        st.rerun()